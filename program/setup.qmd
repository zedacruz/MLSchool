---
title: "Setup Instructions"
---

Here are the steps you need to follow to set up the project:

1. Fork the program's [GitHub Repository](https://github.com/svpino/ml.school) and clone it on your local computer.

1. Create and activate a virtual environment:

    ```bash
    $ python3 -m venv .venv
    $ source .venv/bin/activate
    ```

1. Once the virtual environment is active, you can update `pip` and install the libraries in the `requirements.txt` file:

    ```bash
    $ python -m pip install --upgrade pip
    $ pip install -r requirements.txt
    ```

1. We'll use Jupyter Notebooks during the program. Using the following command, you can install a Jupyter kernel in the virtual environment. If you use Visual Studio Code, you can point your kernel to the virtual environment, and it will install it automatically:
 
    ```bash
    $ python -m ipykernel install --user --name=.venv
    ```
   
1. Install [Docker](https://docs.docker.com/). You'll find installation instructions on their site for your particular environment. After you install it, you can verify Docker is running using the following command:

    ```bash
    $ docker ps
    ```

At this point you can open the project using Visual Studio Code or your favorite IDE. Make sure you point the Jupyter kernel to the virtual environment that you created before.

## Setting up Comet

[Comet](https://www.comet.com/signup?utm_source=svpino_course&utm_medium=partner&utm_content=github) is a cloud-based machine learning platform that allows us to automatically track and version experiments. 

1. Create a free [Comet account](https://www.comet.com/signup?utm_source=svpino_course&utm_medium=partner&utm_content=github) using either your email address or GitHub credentials.

1. Once inside Comet, create a new project.

1. Get your Comet API Key. Check [Comet's quick start guide](https://www.comet.com/docs/v2/guides/getting-started/quickstart/) for information about where to find your API key.

1. Create an `.env` file in the root directory of your local repository and add the following environment variables to it. Make sure you replace the value of each variable with the correct value:

    ```bash
    COMET_API_KEY=[YOUR COMET API KEY]
    COMET_PROJECT_NAME=[YOUR COMET PROJECT NAME]
    ```

## Configuring AWS

If you don't have one yet, create a new AWS account. A community member noticed that indicating the account is for personal use and his interest is in Machine Learning gave him immediate access to the hardware we need for the program.

We'll need access to `ml.m5.xlarge` instances. By default, the quota for a new account is zero, but the tip above might fix this problem. If it doesn't, you'll need to request a quota increase. 

You can do this in your AWS account under Service Quotas > AWS Services > Amazon SageMaker. Find `ml.m5.xlarge` and request a quota increase for processing jobs, training jobs, transform jobs, and endpoint usage. Ask for a minimum of 3 instances.

You'll need access to AWS from your local environment. [Install the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) and [configure it](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html) with your 
`aws_access_key_id` and `aws_secret_access_key.` 

To get an access key, you first need to open the IAM service, find your user, select Security Credentials, then assign a Multi-Factor Authentication (MFA) device and follow the prompts. After setup and verified, you can 
click to create an access key. 

After you finish configuring the CLI, create a new S3 bucket where we will store the data and every resource we are going to create during the program. The name of the bucket must be unique:

```bash
$ aws s3api create-bucket --bucket [YOUR-BUCKET-NAME]
```

:::{.callout-note}
If you want to create a bucket in a region other than **us-east-1**, you need to use the `--create-bucket-configuration` argument to specify your `LocationConstraint`. See the example below:
:::

```bash
$ aws s3api create-bucket --bucket [YOUR-BUCKET-NAME] \
    --create-bucket-configuration LocationConstraint="eu-west-1"
```

Upload the dataset to the S3 bucket you just created:

```bash
$ aws s3 cp program/penguins.csv s3://[YOUR-BUCKET-NAME]/penguins/data/data.csv
```

## Configuring SageMaker

If you don't have one yet, create a SageMaker domain. The [Getting Started on Amazon SageMaker Studio](https://www.youtube.com/watch?v=oBx_o57gDGY) video will walk you through the process. 

After you are done, run the following command to return the Domain Id and the User Profile Name of your SageMaker domain:

```bash
$ aws sagemaker list-user-profiles | grep -E '"DomainId"|"UserProfileName"' \
    | awk -F'[:,"]+' '{print $2":"$3 $4 $5}'
```

Use the `DomainId` and the `UserProfileName` from the response and replace them in the following command that we'll return the execution role attached to the user:

```bash
$ aws sagemaker describe-user-profile \
    --domain-id [YOUR-DOMAIN-ID] \
    --user-profile-name [YOUR-USER-PROFILE-NAME] \
    | grep -E "ExecutionRole" | awk -F'["]' '{print $2": "$4}'
```

Create an `.env` file in the root directory of your repository with the following content. Make sure you replace the value of each variable with the correct value:

```bash
BUCKET=[YOUR-BUCKET-NAME]
DOMAIN_ID=[YOUR-DOMAIN-ID]
USER_PROFILE=[YOUR-USER-PROFILE]
ROLE=[YOUR-EXECUTION-ROLE]
```

Open the Amazon IAM service, find the Execution Role from before and edit the custom Execution Policy assigned to it. Edit the permissions of the Execution Policy and replace them with the JSON below. These permissions will give the Execution Role access to the resources we'll use during the program:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "IAM0",
            "Effect": "Allow",
            "Action": [
                "iam:CreateServiceLinkedRole"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "iam:AWSServiceName": [
                        "autoscaling.amazonaws.com",
                        "ec2scheduled.amazonaws.com",
                        "elasticloadbalancing.amazonaws.com",
                        "spot.amazonaws.com",
                        "spotfleet.amazonaws.com",
                        "transitgateway.amazonaws.com"
                    ]
                }
            }
        },
        {
            "Sid": "IAM1",
            "Effect": "Allow",
            "Action": [
                "iam:CreateRole",
                "iam:DeleteRole",
                "iam:PassRole",
                "iam:AttachRolePolicy",
                "iam:DetachRolePolicy",
                "iam:CreatePolicy"
            ],
            "Resource": "*"
        },
        {
            "Sid": "Lambda",
            "Effect": "Allow",
            "Action": [
                "lambda:CreateFunction",
                "lambda:DeleteFunction",
                "lambda:InvokeFunctionUrl",
                "lambda:InvokeFunction",
                "lambda:UpdateFunctionCode",
                "lambda:InvokeAsync",
                "lambda:AddPermission",
                "lambda:RemovePermission"
            ],
            "Resource": "*"
        },
        {
            "Sid": "SageMaker",
            "Effect": "Allow",
            "Action": [
                "sagemaker:UpdateDomain",
                "sagemaker:UpdateUserProfile"
            ],
            "Resource": "*"
        },
        {
            "Sid": "CloudWatch",
            "Effect": "Allow",
            "Action": [
                "cloudwatch:PutMetricData",
                "cloudwatch:GetMetricData",
                "cloudwatch:DescribeAlarmsForMetric",
                "logs:CreateLogStream",
                "logs:PutLogEvents",
                "logs:CreateLogGroup",
                "logs:DescribeLogStreams"
            ],
            "Resource": "*"
        },
        {
            "Sid": "ECR",
            "Effect": "Allow",
            "Action": [
                "ecr:GetAuthorizationToken",
                "ecr:BatchCheckLayerAvailability",
                "ecr:GetDownloadUrlForLayer",
                "ecr:BatchGetImage"
            ],
            "Resource": "*"
        },
        {
            "Sid": "S3",
            "Effect": "Allow",
            "Action": [
                "s3:CreateBucket",
                "s3:ListBucket",
                "s3:GetBucketLocation",
                "s3:PutObject",
                "s3:GetObject",
                "s3:DeleteObject"
            ],
            "Resource": "arn:aws:s3:::*"
        },
        {
            "Sid": "EventBridge",
            "Effect": "Allow",
            "Action": [
                "events:PutRule",
                "events:PutTargets"
            ],
            "Resource": "*"
        }
    ]
}
```

Finally, find the Trust relationships section under the same Execution Role, edit the configuration, and replace it with the JSON below:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": [
                    "sagemaker.amazonaws.com", 
                    "events.amazonaws.com"
                ]
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
```

## Apple silicon

If your local environment is running on Apple silicon, you need to build a TensorFlow docker image to run some of the pipeline steps on your local computer. This is because SageMaker doesn't provide out-of-the-box TensorFlow images compatible with Apple silicon.

You can build the image running the following command:

```bash
$ docker build -t sagemaker-tensorflow-toolkit-local container/.
```

After building this Docker image, the notebook will automatically use it when running the pipeline in Local Mode on your Mac. There's nothing else you need to do.

## Running the code in SageMaker Studio

If you are planning to run the code from inside SageMaker Studio, you will need to create a Lifecycle Configuration to update the kernel. 
To do this, you need to run the `studio-setup.ipynb` notebook once inside SageMaker Studio. After doing this, you can use the **TensorFlow 2.11 Python 3.9 CPU Optimized** kernel with the start-up script named `ml-school.`